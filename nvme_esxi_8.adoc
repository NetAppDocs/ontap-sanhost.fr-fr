---
sidebar: sidebar 
permalink: nvme_esxi_8.html 
keywords: nvme, esxi, ontap, nvme/fc, hypervisor 
summary: 'Vous pouvez configurer NVMe over Fabrics (NVMe-of) sur les hôtes initiateurs exécutant ESXi 8.x et ONTAP comme cible.' 
---
= Configuration d'hôte NVMe-of pour ESXi 8.x avec ONTAP
:hardbreaks:
:toclevels: 1
:allow-uri-read: 
:toclevels: 1
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./media/


[role="lead"]
Vous pouvez configurer NVMe over Fabrics (NVMe-of) sur les hôtes initiateurs exécutant ESXi 8.x et ONTAP comme cible.



== Prise en charge

* Depuis la version ONTAP 9.10.1, le protocole NVMe/TCP est pris en charge pour ONTAP.
* À partir de la version ONTAP 9.9.1 P3, le protocole NVMe/FC est pris en charge pour ESXi 8 et les versions ultérieures.




== Caractéristiques

* Les hôtes initiateurs ESXi peuvent exécuter le trafic NVMe/FC et FCP via les mêmes ports d'adaptateur. Voir la link:https://hwu.netapp.com/Home/Index["Hardware Universe"^] Pour obtenir la liste des contrôleurs et adaptateurs FC pris en charge, Voir la link:https://mysupport.netapp.com/matrix/["Matrice d'interopérabilité NetApp"^] pour obtenir la liste la plus récente des configurations et versions prises en charge.
* Pour ESXi 8.0 et les versions ultérieures, HPP (plug-in haute performance) est le plug-in par défaut pour les périphériques NVMe.




== Limites connues

* Le mappage RDM n'est pas pris en charge.




== Activation de NVMe/FC

NVMe/FC est activé par défaut dans les versions de vSphere.

.Vérifiez le NQN de l'hôte
Vous devez vérifier la chaîne NQN de l'hôte VMware ESXi et vérifier qu'elle correspond à la chaîne NQN de l'hôte pour le sous-système correspondant sur la baie ONTAP.

[listing]
----
# esxcli nvme info get
----
Exemple de résultat :

[listing]
----
Host NQN: nqn.2014-08.org.nvmexpress:uuid:62a19711-ba8c-475d-c954-0000c9f1a436
----
[listing]
----
# vserver nvme subsystem host show -vserver nvme_fc
----
Exemple de résultat :

[listing]
----
Vserver Subsystem Host NQN
------- --------- ----------------------------------------------------------
nvme_fc nvme_ss  nqn.2014-08.org.nvmexpress:uuid:62a19711-ba8c-475d-c954-0000c9f1a436
----
Si les chaînes NQN hôte ne correspondent pas, vous devez utiliser le `vserver nvme subsystem host add` Commande permettant de mettre à jour la chaîne NQN hôte correcte sur votre sous-système ONTAP NVMe correspondant.



== Configurez Broadcom/Emulex et Marvell/Qlogic

Le `lpfc` le conducteur et le `qlnativefc` La fonctionnalité NVMe/FC est activée par défaut dans vSphere 8.x du pilote.

Voir link:https://mysupport.netapp.com/matrix/["Matrice d'interopérabilité NetApp"^] pour vérifier si la configuration est prise en charge avec le pilote ou le micrologiciel.



== Validation de la spécification NVMe/FC

La procédure suivante permet de valider NVMe/FC.

.Étapes
. Vérifiez que l'adaptateur NVMe/FC est répertorié sur l'hôte ESXi :
+
[listing]
----
# esxcli nvme adapter list
----
+
Exemple de résultat :

+
[listing]
----

Adapter  Adapter Qualified Name           Transport Type  Driver      Associated Devices
-------  -------------------------------  --------------  ----------  ------------------
vmhba64  aqn:lpfc:100000109b579f11        FC              lpfc
vmhba65  aqn:lpfc:100000109b579f12        FC              lpfc
vmhba66  aqn:qlnativefc:2100f4e9d456e286  FC              qlnativefc
vmhba67  aqn:qlnativefc:2100f4e9d456e287  FC              qlnativefc
----
. Vérifier que les namespaces NVMe/FC sont correctement créés :
+
Les UID dans l'exemple suivant représentent les périphériques d'espace de noms NVMe/FC.

+
[listing, subs="+quotes"]
----
# esxcfg-mpath -b
uuid.116cb7ed9e574a0faf35ac2ec115969d : NVMe Fibre Channel Disk (*uuid.116cb7ed9e574a0faf35ac2ec115969d*)
   vmhba64:C0:T0:L5 LUN:5 state:active fc Adapter: WWNN: 20:00:00:24:ff:7f:4a:50 WWPN: 21:00:00:24:ff:7f:4a:50  Target: WWNN: 20:04:d0:39:ea:3a:b2:1f WWPN: 20:05:d0:39:ea:3a:b2:1f
   vmhba64:C0:T1:L5 LUN:5 state:active fc Adapter: WWNN: 20:00:00:24:ff:7f:4a:50 WWPN: 21:00:00:24:ff:7f:4a:50  Target: WWNN: 20:04:d0:39:ea:3a:b2:1f WWPN: 20:07:d0:39:ea:3a:b2:1f
   vmhba65:C0:T1:L5 LUN:5 state:active fc Adapter: WWNN: 20:00:00:24:ff:7f:4a:51 WWPN: 21:00:00:24:ff:7f:4a:51  Target: WWNN: 20:04:d0:39:ea:3a:b2:1f WWPN: 20:08:d0:39:ea:3a:b2:1f
   vmhba65:C0:T0:L5 LUN:5 state:active fc Adapter: WWNN: 20:00:00:24:ff:7f:4a:51 WWPN: 21:00:00:24:ff:7f:4a:51  Target: WWNN: 20:04:d0:39:ea:3a:b2:1f WWPN: 20:06:d0:39:ea:3a:b2:1f
----
+
[NOTE]
====
Dans ONTAP 9.7, la taille de bloc par défaut d'un namespace NVMe/FC est de 4 Ko. Cette taille par défaut n'est pas compatible avec ESXi. Par conséquent, lorsque vous créez des espaces de noms pour ESXi, vous devez définir la taille du bloc d'espace de noms sur *512B*. Vous pouvez le faire en utilisant le `vserver nvme namespace create` commande.

Exemple

`vserver nvme namespace create -vserver vs_1 -path /vol/nsvol/namespace1 -size 100g -ostype vmware -block-size 512B`

Reportez-vous à la link:https://docs.netapp.com/us-en/ontap/concepts/manual-pages.html["Pages de manuel de commande ONTAP 9"^] pour plus d'informations.

====
. Vérifiez l'état des chemins ANA individuels des périphériques d'espace de noms NVMe/FC respectifs :
+
[listing, subs="+quotes"]
----
# esxcli storage hpp path list -d uuid.df960bebb5a74a3eaaa1ae55e6b3411d

fc.20000024ff7f4a50:21000024ff7f4a50-fc.2004d039ea3ab21f:2005d039ea3ab21f-uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Runtime Name: vmhba64:C0:T0:L3
   Device: uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Device Display Name: NVMe Fibre Channel Disk (uuid.df960bebb5a74a3eaaa1ae55e6b3411d)
   Path State: active unoptimized
   Path Config: {ANA_GRP_id=4,*ANA_GRP_state=ANO*,health=UP}

fc.20000024ff7f4a51:21000024ff7f4a51-fc.2004d039ea3ab21f:2008d039ea3ab21f-uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Runtime Name: vmhba65:C0:T1:L3
   Device: uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Device Display Name: NVMe Fibre Channel Disk (uuid.df960bebb5a74a3eaaa1ae55e6b3411d)
   Path State: active
   Path Config: {ANA_GRP_id=4,*ANA_GRP_state=AO*,health=UP}

fc.20000024ff7f4a51:21000024ff7f4a51-fc.2004d039ea3ab21f:2006d039ea3ab21f-uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Runtime Name: vmhba65:C0:T0:L3
   Device: uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Device Display Name: NVMe Fibre Channel Disk (uuid.df960bebb5a74a3eaaa1ae55e6b3411d)
   Path State: active unoptimized
   Path Config: {ANA_GRP_id=4,*ANA_GRP_state=ANO*,health=UP}

fc.20000024ff7f4a50:21000024ff7f4a50-fc.2004d039ea3ab21f:2007d039ea3ab21f-uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Runtime Name: vmhba64:C0:T1:L3
   Device: uuid.df960bebb5a74a3eaaa1ae55e6b3411d
   Device Display Name: NVMe Fibre Channel Disk (uuid.df960bebb5a74a3eaaa1ae55e6b3411d)
   Path State: active
   Path Config: {ANA_GRP_id=4,*ANA_GRP_state=AO*,health=UP}

----




== Configurez NVMe/TCP

Dans ESXi 8.x, les modules NVMe/TCP requis sont chargés par défaut. Pour configurer le réseau et l'adaptateur NVMe/TCP, reportez-vous à la documentation de VMware vSphere.



== Validation du protocole NVMe/TCP

La procédure suivante permet de valider NVMe/TCP.

.Étapes
. Vérifiez le statut de l'adaptateur NVMe/TCP :
+
[listing]
----
esxcli nvme adapter list
----
+
Exemple de résultat :

+
[listing]
----
Adapter  Adapter Qualified Name           Transport Type  Driver   Associated Devices
-------  -------------------------------  --------------  -------  ------------------
vmhba65  aqn:nvmetcp:ec-2a-72-0f-e2-30-T  TCP             nvmetcp  vmnic0
vmhba66  aqn:nvmetcp:34-80-0d-30-d1-a0-T  TCP             nvmetcp  vmnic2
vmhba67  aqn:nvmetcp:34-80-0d-30-d1-a1-T  TCP             nvmetcp  vmnic3
----
. Récupérer la liste des connexions NVMe/TCP :
+
[listing]
----
esxcli nvme controller list
----
+
Exemple de résultat :

+
[listing]
----
Name                                                  Controller Number  Adapter  Transport Type  Is Online  Is VVOL
---------------------------------------------------------------------------------------------------------  -----------------  -------
nqn.2014-08.org.nvmexpress.discovery#vmhba64#192.168.100.166:8009  256  vmhba64  TCP                  true    false
nqn.1992-08.com.netapp:sn.89bb1a28a89a11ed8a88d039ea263f93:subsystem.nvme_ss#vmhba64#192.168.100.165:4420 258  vmhba64  TCP  true    false
nqn.1992-08.com.netapp:sn.89bb1a28a89a11ed8a88d039ea263f93:subsystem.nvme_ss#vmhba64#192.168.100.168:4420 259  vmhba64  TCP  true    false
nqn.1992-08.com.netapp:sn.89bb1a28a89a11ed8a88d039ea263f93:subsystem.nvme_ss#vmhba64#192.168.100.166:4420 260  vmhba64  TCP  true    false
nqn.2014-08.org.nvmexpress.discovery#vmhba64#192.168.100.165:8009  261  vmhba64  TCP                  true    false
nqn.2014-08.org.nvmexpress.discovery#vmhba65#192.168.100.155:8009  262  vmhba65  TCP                  true    false
nqn.1992-08.com.netapp:sn.89bb1a28a89a11ed8a88d039ea263f93:subsystem.nvme_ss#vmhba64#192.168.100.167:4420 264  vmhba64  TCP  true    false

----
. Récupérer la liste du nombre de chemins d'accès à un namespace NVMe :
+
[listing, subs="+quotes"]
----
esxcli storage hpp path list -d *uuid.f4f14337c3ad4a639edf0e21de8b88bf*
----
+
Exemple de résultat :

+
[listing, subs="+quotes"]
----
tcp.vmnic2:34:80:0d:30:ca:e0-tcp.192.168.100.165:4420-uuid.f4f14337c3ad4a639edf0e21de8b88bf
   Runtime Name: vmhba64:C0:T0:L5
   Device: uuid.f4f14337c3ad4a639edf0e21de8b88bf
   Device Display Name: NVMe TCP Disk (uuid.f4f14337c3ad4a639edf0e21de8b88bf)
   Path State: active
   Path Config: {ANA_GRP_id=6,*ANA_GRP_state=AO*,health=UP}

tcp.vmnic2:34:80:0d:30:ca:e0-tcp.192.168.100.168:4420-uuid.f4f14337c3ad4a639edf0e21de8b88bf
   Runtime Name: vmhba64:C0:T3:L5
   Device: uuid.f4f14337c3ad4a639edf0e21de8b88bf
   Device Display Name: NVMe TCP Disk (uuid.f4f14337c3ad4a639edf0e21de8b88bf)
   Path State: active unoptimized
   Path Config: {ANA_GRP_id=6,*ANA_GRP_state=ANO*,health=UP}

tcp.vmnic2:34:80:0d:30:ca:e0-tcp.192.168.100.166:4420-uuid.f4f14337c3ad4a639edf0e21de8b88bf
   Runtime Name: vmhba64:C0:T2:L5
   Device: uuid.f4f14337c3ad4a639edf0e21de8b88bf
   Device Display Name: NVMe TCP Disk (uuid.f4f14337c3ad4a639edf0e21de8b88bf)
   Path State: active unoptimized
   Path Config: {ANA_GRP_id=6,*ANA_GRP_state=ANO*,health=UP}

tcp.vmnic2:34:80:0d:30:ca:e0-tcp.192.168.100.167:4420-uuid.f4f14337c3ad4a639edf0e21de8b88bf
   Runtime Name: vmhba64:C0:T1:L5
   Device: uuid.f4f14337c3ad4a639edf0e21de8b88bf
   Device Display Name: NVMe TCP Disk (uuid.f4f14337c3ad4a639edf0e21de8b88bf)
   Path State: active
   Path Config: {ANA_GRP_id=6,*ANA_GRP_state=AO*,health=UP}
----




== Problèmes connus

La configuration hôte NVMe-of pour ESXi 8.x avec ONTAP présente les problèmes connus suivants :

[cols="10,30,30"]
|===
| ID de bug NetApp | Titre | Description 


| link:https://mysupport.netapp.com/site/bugs-online/product/ONTAP/BURT/1420654["1420654"^] | Nœud ONTAP non opérationnel lorsque le protocole NVMe/FC est utilisé avec ONTAP version 9.9.1 | ONTAP 9.9.1 a introduit la prise en charge de la commande « abort » NVMe. Lorsque ONTAP reçoit la commande « abort » pour abandonner une commande fusionnée NVMe en attente de sa commande partenaire, une interruption du nœud ONTAP se produit. Le problème est remarqué uniquement avec les hôtes qui utilisent des commandes fusionnées NVMe (par exemple, ESX) et un transport Fibre Channel (FC). 


| 1543660 | Une erreur d'E/S se produit lorsque les machines virtuelles Linux utilisant des adaptateurs vNVMe rencontrent une fenêtre long All paths Down (APD)  a| 
Les machines virtuelles Linux exécutant vSphere 8.x et versions ultérieures et utilisant des adaptateurs virtuels NVMe (vNVME) rencontrent une erreur d'E/S, car l'opération de nouvelle tentative vNVMe est désactivée par défaut. Pour éviter une interruption sur les machines virtuelles Linux exécutant des noyaux plus anciens lors d'une panne de tous les chemins (APD) ou d'une charge d'E/S importante, VMware a introduit un « VSCSIDisableNvmeRetry » ajustable pour désactiver l'opération de nouvelle tentative vNVMe.

|===
.Informations associées
link:https://docs.netapp.com/us-en/netapp-solutions/virtualization/vsphere_ontap_ontap_for_vsphere.html["Tr-4597-VMware vSphere avec ONTAP"^]
link:https://kb.vmware.com/s/article/2031038["Prise en charge de VMware vSphere 5.x, 6.x et 7.x avec NetApp MetroCluster (2031038)"^]
link:https://kb.vmware.com/s/article/83370["Prise en charge de VMware vSphere 6.x et 7.x avec la synchronisation active NetApp SnapMirror"^]
