---
sidebar: sidebar 
permalink: nvme-rockylinux-85.html 
keywords: nvme, linux, rocky linux, enterprise 
summary: 'Comment configurer l"hôte NVMe-oF pour Rocky Linux 8.5 avec ONTAP' 
---
= Configurer Rocky Linux 8.5 avec NVMe-oF pour le stockage ONTAP
:hardbreaks:
:toclevels: 1
:allow-uri-read: 
:toclevels: 1
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./media/


[role="lead"]
Les hôtes Rocky Linux 8.5 prennent en charge les protocoles NVMe/FC et NVMe/TCP avec l'accès asymétrique aux espaces de noms (ANA). ANA est équivalent à l'accès asymétrique aux unités logiques (ALUA) dans les environnements iSCSI et FCP et est implémenté à l'aide de la fonctionnalité NVMe multipath intégrée au noyau.

Pour plus de détails sur les configurations prises en charge, consultez le link:https://mysupport.netapp.com/matrix/["Matrice d'interopérabilité"^].

.Description de la tâche
Vous pouvez utiliser le support et les fonctionnalités suivants avec la configuration d’hôte NVMe-oF pour Rocky Linux 8.5. Vous devez également vérifier les limites connues avant de commencer le processus de configuration.

* Support disponible :
+
** Prise en charge de NVMe over TCP (NVMe/TCP) et de NVMe over Fibre Channel (NVMe/FC). Le plug-in NetApp du pack natif `nvme-cli` affiche les détails des ONTAP pour les namespaces NVMe/FC et NVMe/TCP.
** Exécution du trafic NVMe et SCSI sur le même hôte Par exemple, vous pouvez configurer dm-multipath pour les périphériques SCSI mpath sur les LUN SCSI et utiliser NVMe multipath pour configurer les périphériques d'espace de noms NVMe-oF sur l'hôte.


* Limitations connues :
+
** Le multipath NVMe dans le noyau est désactivé par défaut pour les hôtes Rocky Linux 8.5 NVMe-oF. Par conséquent, vous devez l'activer manuellement.
** Sur les hôtes Rocky Linux 8.5, NVMe/TCP est une fonctionnalité d'aperçu technologique en raison de problèmes ouverts. Reportez-vous au https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html-single/8.5_release_notes/index#technology-preview_file-systems-and-storage["Notes de version de Rocky Linux 8.5"^] pour plus de détails.
** Le démarrage SAN à l'aide du protocole NVMe-of n'est pas pris en charge pour le moment.






== Étape 1 : activez éventuellement le démarrage SAN

Vous pouvez configurer votre hôte pour qu'il utilise le démarrage SAN afin de simplifier le déploiement et d'améliorer l'évolutivité.

.Avant de commencer
Utilisez le link:https://mysupport.netapp.com/matrix/#welcome["Matrice d'interopérabilité"^] pour vérifier que votre système d'exploitation Linux, votre adaptateur de bus hôte (HBA), le micrologiciel HBA, le BIOS de démarrage HBA et la version ONTAP prennent en charge le démarrage SAN.

.Étapes
. https://docs.netapp.com/us-en/ontap/san-admin/create-nvme-namespace-subsystem-task.html["Créez un espace de noms de démarrage SAN et mappez-le à l'hôte"^].
. Activez le démarrage SAN dans le BIOS du serveur pour les ports auxquels l'espace de noms de démarrage SAN est mappé.
+
Pour plus d'informations sur l'activation du BIOS HBA, reportez-vous à la documentation spécifique au fournisseur.

. Vérifiez que la configuration a réussi en redémarrant l'hôte et en vérifiant que le système d'exploitation est opérationnel.




== Étape 2 : Valider les versions du logiciel

Utilisez la procédure suivante pour valider les versions minimales du logiciel Rocky Linux 8.5 prises en charge.

.Étapes
. Installez Rocky Linux 8.5 sur le serveur. Une fois l'installation terminée, vérifiez que vous utilisez le noyau Rocky Linux 8.5 spécifié :
+
[source, cli]
----
uname -r
----
+
L'exemple suivant montre une version du noyau Rocky Linux :

+
[listing]
----
5.14.0-570.12.1.el9_6.x86_64
----
. Installer le `nvme-cli` groupe :
+
[source, cli]
----
rpm -qa|grep nvme-cli
----
+
L'exemple suivant montre une version de package nvme-cli :

+
[listing]
----
nvme-cli-2.11-5.el9.x86_64
----
. Installer le `libnvme` groupe :
+
[source, cli]
----
rpm -qa|grep libnvme
----
+
L'exemple suivant montre un  `libnvme` version du paquet :

+
[listing]
----
libnvme-1.11.1-1.el9.x86_64
----
. Sur l'hôte Rocky Linux, vérifiez la chaîne hostnqn à  `/etc/nvme/hostnqn` :
+
[source, cli]
----
cat /etc/nvme/hostnqn
----
+
L'exemple suivant montre un  `hostnqn` version:

+
[listing]
----
nqn.2014-08.org.nvmexpress:uuid:4c4c4544-0056-5410-8048-b9c04f425633
----
. Vérifiez que le `hostnqn` la chaîne correspond au `hostnqn` Chaîne du sous-système correspondant sur la baie ONTAP :
+
[source, cli]
----
::> vserver nvme subsystem host show -vserver vs_coexistence_LPE36002
----
+
.Montrer l'exemple
[%collapsible]
====
[listing]
----
Vserver Subsystem Priority  Host NQN
------- --------- --------  ------------------------------------------------
vs_coexistence_LPE36002
        nvme
                  regular   nqn.2014-08.org.nvmexpress:uuid:4c4c4544-0056-5410-8048-b9c04f425633
        nvme_1
                  regular   nqn.2014-08.org.nvmexpress:uuid:4c4c4544-0056-5410-8048-b9c04f425633
        nvme_2
                  regular   nqn.2014-08.org.nvmexpress:uuid:4c4c4544-0056-5410-8048-b9c04f425633
        nvme_3
                  regular   nqn.2014-08.org.nvmexpress:uuid:4c4c4544-0056-5410-8048-b9c04f425633
4 entries were displayed.
----
====
+

NOTE: Si le `hostnqn` les chaînes ne correspondent pas, utilisez le `vserver modify` commande pour mettre à jour le `hostnqn` Chaîne sur le sous-système de matrice ONTAP correspondant à `hostnqn` chaîne de `/etc/nvme/hostnqn` sur l'hôte.





== Étape 3 : Configurer NVMe/FC

Vous pouvez configurer NVMe/FC avec les adaptateurs FC Broadcom/Emulex ou Marvell/Qlogic. Vous devez également découvrir manuellement les sous-systèmes et espaces de noms NVMe/TCP.

[role="tabbed-block"]
====
.Broadcom/Emulex
Configuration de NVMe/FC pour une carte Broadcom/Emulex

--
.Étapes
. Vérifiez que vous utilisez le modèle d'adaptateur pris en charge :
+
.. Afficher les noms des modèles :
+
[source, cli]
----
cat /sys/class/scsi_host/host*/modelname
----
+
Vous devriez voir le résultat suivant :

+
[listing]
----
LPe36002-M64
LPe36002-M64
----
.. Afficher les descriptions des modèles :
+
[source, cli]
----
cat /sys/class/scsi_host/host*/modeldesc
----
+
Vous devriez voir une sortie similaire à l’exemple suivant :

+
[listing]
----
Emulex LightPulse LPe36002-M64 2-Port 64Gb Fibre Channel Adapter
Emulex LightPulse LPe36002-M64 2-Port 64Gb Fibre Channel Adapter
----


. Vérifiez que vous utilisez la carte Broadcom recommandée `lpfc` micrologiciel et pilote de boîte de réception :
+
.. Afficher la version du firmware :
+
[source, cli]
----
cat /sys/class/scsi_host/host*/fwrev
----
+
L'exemple suivant montre les versions du firmware :

+
[listing]
----
14.4.317.10, sli-4:6:d
14.4.317.10, sli-4:6:d
----
.. Afficher la version du pilote de la boîte de réception :
+
[source, cli]
----
cat /sys/module/lpfc/version`
----
+
L'exemple suivant montre une version de pilote :

+
[listing]
----
0:14.4.0.2
----


+
Pour obtenir la liste actuelle des versions de pilotes et de micrologiciels de carte prises en charge, consultez le link:https://mysupport.netapp.com/matrix/["Matrice d'interopérabilité"^].

. Vérifiez que la sortie attendue de `lpfc_enable_fc4_type` est définie sur `3`:
+
[source, cli]
----
cat /sys/module/lpfc/parameters/lpfc_enable_fc4_type
----
. Vérifiez que vous pouvez afficher vos ports initiateurs :
+
[source, cli]
----
cat /sys/class/fc_host/host*/port_name
----
+
L'exemple suivant montre les identités de port :

+
[listing]
----
0x100000109bf044b1
0x100000109bf044b2
----
. Vérifiez que vos ports initiateurs sont en ligne :
+
[source, cli]
----
cat /sys/class/fc_host/host*/port_state
----
+
Vous devriez voir le résultat suivant :

+
[listing]
----
Online
Online
----
. Vérifiez que les ports initiateurs NVMe/FC sont activés et que les ports cibles sont visibles :
+
[source, cli]
----
cat /sys/class/scsi_host/host*/nvme_info
----
+
.Montrer l'exemple
[%collapsible]
=====
[listing, subs="+quotes"]
----
NVME Initiator Enabled
XRI Dist lpfc2 Total 6144 IO 5894 ELS 250
NVME LPORT lpfc2 WWPN x100000109bf044b1 WWNN x200000109bf044b1 DID x022a00 *ONLINE*
NVME RPORT       WWPN x202fd039eaa7dfc8 WWNN x202cd039eaa7dfc8 DID x021310 *TARGET DISCSRVC ONLINE*
NVME RPORT       WWPN x202dd039eaa7dfc8 WWNN x202cd039eaa7dfc8 DID x020b10 *TARGET DISCSRVC ONLINE*

NVME Statistics
LS: Xmt 0000000810 Cmpl 0000000810 Abort 00000000
LS XMIT: Err 00000000  CMPL: xb 00000000 Err 00000000
Total FCP Cmpl 000000007b098f07 Issue 000000007aee27c4 OutIO ffffffffffe498bd
        abort 000013b4 noxri 00000000 nondlp 00000058 qdepth 00000000 wqerr 00000000 err 00000000
FCP CMPL: xb 000013b4 Err 00021443

NVME Initiator Enabled
XRI Dist lpfc3 Total 6144 IO 5894 ELS 250
NVME LPORT lpfc3 WWPN x100000109bf044b2 WWNN x200000109bf044b2 DID x021b00 *ONLINE*
NVME RPORT       WWPN x2033d039eaa7dfc8 WWNN x202cd039eaa7dfc8 DID x020110 *TARGET DISCSRVC ONLINE*
NVME RPORT       WWPN x2032d039eaa7dfc8 WWNN x202cd039eaa7dfc8 DID x022910 *TARGET DISCSRVC ONLINE*

NVME Statistics
LS: Xmt 0000000840 Cmpl 0000000840 Abort 00000000
LS XMIT: Err 00000000  CMPL: xb 00000000 Err 00000000
Total FCP Cmpl 000000007afd4434 Issue 000000007ae31b83 OutIO ffffffffffe5d74f
        abort 000014a5 noxri 00000000 nondlp 0000006a qdepth 00000000 wqerr 00000000 err 00000000
FCP CMPL: xb 000014a5 Err 0002149a
----
=====


--
.Marvell/QLogic
--
Configuration du NVMe/FC pour un adaptateur Marvell/QLogic


NOTE: Le pilote natif de la boîte de réception qla2xxx inclus dans le noyau Rocky Linux dispose des derniers correctifs. Ces correctifs sont essentiels à la prise en charge de ONTAP.

.Étapes
. Vérifiez que vous exécutez les versions du pilote de carte et du micrologiciel prises en charge :
+
[source, cli]
----
cat /sys/class/fc_host/host*/symbolic_name
----
+
L'exemple suivant montre les versions du pilote et du micrologiciel :

+
[listing]
----
QLE2742 FW:v9.14.00 DVR:v10.02.09.200-k
QLE2742 FW:v9.14.00 DVR:v10.02.09.200-k
----
. Vérifiez-le `ql2xnvmeenable` est défini. L'adaptateur Marvell peut ainsi fonctionner en tant qu'initiateur NVMe/FC :
+
[source, cli]
----
cat /sys/module/qla2xxx/parameters/ql2xnvmeenable
----
+
La sortie attendue est 1.



--
====


== Étape 4 : Activez éventuellement 1 Mo d'E/S

Vous pouvez activer des requêtes d'E/S de 1 Mo pour NVMe/FC configuré avec un adaptateur Broadcom. ONTAP signale une taille maximale de transfert de données (MDTS) de 8 dans les données du contrôleur d'identification. La taille maximale des demandes d'E/S peut donc atteindre 1 Mo. Pour émettre des requêtes d'E/S de 1 Mo, vous devez augmenter la valeur lpfc du paramètre.  `lpfc_sg_seg_cnt` paramètre à 256 à partir de la valeur par défaut de 64.


NOTE: Ces étapes ne s'appliquent pas aux hôtes NVMe/FC Qlogic.

.Étapes
. Réglez le `lpfc_sg_seg_cnt` paramètre sur 256 :
+
[listing]
----
cat /etc/modprobe.d/lpfc.conf
----
+
[listing]
----
options lpfc lpfc_sg_seg_cnt=256
----
. Exécutez `dracut -f` la commande et redémarrez l'hôte.
. Vérifier que la valeur de `lpfc_sg_seg_cnt` est 256 :
+
[listing]
----
cat /sys/module/lpfc/parameters/lpfc_sg_seg_cnt
----




== Étape 5 : Configurer NVMe/TCP

Le protocole NVMe/TCP ne prend pas en charge `auto-connect` l'opération. Vous pouvez à la place détecter les sous-systèmes et les espaces de noms NVMe/TCP en exécutant manuellement les opérations NVMe/TCP `connect` ou `connect-all`.



== Étape 6 : Valider NVMe-oF

Vérifiez que l'état des chemins d'accès multiples NVMe in-kernel, l'état ANA et les namespaces ONTAP sont corrects pour la configuration NVMe-of.

.Étapes
. Vérifiez que le chemin d'accès multiples NVMe intégré au noyau est activé :
+
[source, cli]
----
cat /sys/module/nvme_core/parameters/multipath
----
+
Vous devriez voir le résultat suivant :

+
[listing]
----
Y
----
. Vérifiez que les paramètres NVMe-of appropriés (par exemple, modèle défini sur contrôleur NetApp ONTAP et iopole d'équilibrage de la charge sur round-Robin) pour les espaces de noms ONTAP respectifs reflètent correctement l'hôte :
+
.. Afficher les sous-systèmes :
+
[source, cli]
----
cat /sys/class/nvme-subsystem/nvme-subsys*/model
----
+
Vous devriez voir le résultat suivant :

+
[listing]
----
NetApp ONTAP Controller
NetApp ONTAP Controller
----
.. Afficher la politique :
+
[source, cli]
----
cat /sys/class/nvme-subsystem/nvme-subsys*/iopolicy
----
+
Vous devriez voir le résultat suivant :

+
[listing]
----
round-robin
round-robin
----


. Vérifiez que les espaces de noms sont créés et correctement découverts sur l'hôte :
+
[source, cli]
----
nvme list
----
+
.Montrer l'exemple
[%collapsible]
====
[listing]
----
Node         SN                   Model
---------------------------------------------------------
/dev/nvme4n1 81Ix2BVuekWcAAAAAAAB	NetApp ONTAP Controller


Namespace Usage    Format             FW             Rev
-----------------------------------------------------------
1                 21.47 GB / 21.47 GB	4 KiB + 0 B   FFFFFFFF
----
====




== Étape 7 : passez en revue les problèmes connus

Aucun problème connu n'existe pour la configuration de l'hôte NVMe-oF sur Rocky Linux 8.5 avec la version ONTAP.
