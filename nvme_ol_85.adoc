---
sidebar: sidebar 
permalink: nvme_ol_85.html 
keywords: nvme, linux, oracle, 8.5 
summary: 'Configuration d"hôte VME/FC pour Oracle Linux 8.5 avec ONTAP, par exemple' 
---
= Configuration d'hôte NVMe/FC pour Oracle Linux 8.5 avec ONTAP
:toc: macro
:hardbreaks:
:toclevels: 1
:allow-uri-read: 
:toc: 
:toclevels: 1
:nofooter: 
:icons: font
:linkattrs: 
:imagesdir: ./media/
:toc-position: content




== Prise en charge

NVMe/FC est pris en charge sur ONTAP 9.6 ou version ultérieure pour Oracle Linux 8.5. L'hôte Oracle Linux 8.5 peut exécuter à la fois le trafic NVMe/FC et FCP via les mêmes ports d'adaptateur d'initiateur Fibre Channel (FC). Voir la link:https://hwu.netapp.com/["Hardware Universe"^] Pour obtenir la liste des contrôleurs et adaptateurs FC pris en charge, Pour obtenir la liste la plus récente des configurations prises en charge, consultez le link:https://mysupport.netapp.com/matrix/["Matrice d'interopérabilité NetApp"^].


NOTE: Vous pouvez utiliser les paramètres de configuration fournis dans ce contenu pour configurer les clients cloud connectés à link:https://docs.netapp.com/us-en/cloud-manager-cloud-volumes-ontap/index.html["Cloud Volumes ONTAP"^] et link:https://docs.netapp.com/us-en/cloud-manager-fsx-ontap/index.html["Amazon FSX pour ONTAP"^].



== Caractéristiques

* Oracle Linux 8.5 a des chemins d'accès multiples NVMe dans le noyau activés par défaut pour les espaces de noms NVMe.
* Avec Oracle Linux 8.5, `nvme-fc auto-connect` des scripts sont inclus dans le stockage natif `nvme-cli` création de package. Vous pouvez donc vous appuyer sur ces scripts natifs de connexion automatique au lieu d'installer les scripts de connexion automatique fournis par le fournisseur externe.
* Avec Oracle Linux 8.5, une fonctionnalité native `udev` la règle est déjà fournie dans le cadre du `nvme-cli` Une solution qui permet l'équilibrage de la charge Round-Robin pour les chemins d'accès multiples NVMe. Vous n'avez donc plus besoin de créer cette règle manuellement (comme cela a été fait dans Oracle Linux 8.1).
* Avec Oracle Linux 8.5, le trafic NVMe et SCSI peut être exécuté sur le même hôte existant. En fait, ce devrait être la configuration hôte couramment déployée. Pour SCSI, vous pouvez donc le configurer `dm-multipath` Comme d'habitude pour les LUN SCSI ce qui entraîne des périphériques mpath, tandis que le protocole NVMe multivoie peut être utilisé pour configurer des périphériques NVMe-of multipathing (c'est-à-dire, `/dev/nvmeXnY`) sur l'hôte.
* Avec Oracle Linux 8.5, le plug-in NetApp natif `nvme-cli` Est capable d'afficher les détails de ONTAP ainsi que les espaces de noms ONTAP.




== Limites connues

* NVME/TCP est une fonctionnalité de prévisualisation de la technologie et, par conséquent, la prise en charge n'est pas publiée sur IMT.
* Pour passer confortablement en cas d'événements de basculement ONTAP, tels que les basculements de stockage pour NVMe/TCP, il est recommandé de définir une période de nouvelle tentative plus longue, comme par exemple 30 minutes, en ajustant le `ctrl_loss_tmo` minuterie. Voir les détails suivants.




== Configuration requise

Reportez-vous à la link:https://mysupport.netapp.com/matrix/["Matrice d'interopérabilité NetApp"^] pour obtenir des détails précis sur les configurations prises en charge.



== Activation de NVMe/FC avec Oracle Linux 8.5

.Étapes
. Installez Oracle Linux 8.5 General Availability (GA) sur le serveur. Une fois l'installation terminée, vérifiez que vous exécutez le noyau Oracle Linux 8.5 GA spécifié. Voir la link:https://mysupport.netapp.com/matrix/["Matrice d'interopérabilité NetApp"^] pour obtenir la liste la plus récente des versions prises en charge.
+
[listing]
----
# uname -r
5.4.17-2136.309.4.el8uek.x86_64
----
. Installer le `nvme-cli` création de package.
+
[listing]
----
# rpm -qa|grep nvme-cli
nvme-cli-1.14-3.el8.x86_64
----
. Sur l'hôte Oracle Linux 8.5, vérifiez le `hostnqn` chaîne à `/etc/nvme/hostnqn` et vérifier qu'il correspond au `hostnqn` Chaîne du sous-système correspondant sur la matrice ONTAP.
+
[listing]
----
# cat /etc/nvme/hostnqn
nqn.2014-08.org.nvmexpress:uuid:9ed5b327-b9fc-4cf5-97b3-1b5d986345d1
::> vserver nvme subsystem host show -vserver vs_ol_nvme

Vserver    Subsystem      Host NQN
---------------------------------------------
vs_ol_nvme nvme_ss_ol_1   nqn.2014-08.org.nvmexpress:uuid:9ed5b327-b9fc-4cf5-97b3-1b5d986345d1
----
+

NOTE: Si le `hostnqn` les chaînes ne correspondent pas, vous devez utiliser le `vserver modify` commande pour mettre à jour le `hostnqn` Chaîne du sous-système de la matrice ONTAP correspondant à la `hostnqn` chaîne de `/etc/nvme/hostnqn` sur l'hôte.

. Redémarrez l'hôte.
+
[NOTE]
====
Si vous prévoyez d'exécuter à la fois le trafic NVMe et SCSI sur le même hôte existant Oracle Linux 8.5, il est recommandé d'utiliser les chemins d'accès multiples NVMe in-kernel pour les espaces de noms ONTAP et `dm-multipath` Pour les LUN ONTAP respectivement. Et cela signifie également que les espaces de noms ONTAP devraient être noirci dans `dm-multipath` pour éviter `dm-multipath` en réclamant ces périphériques d'espace de noms. Pour ce faire, ajoutez le `enable_foreign` réglage sur `/etc/multipath.conf`:

[listing]
----
# cat /etc/multipath.conf
defaults {
        enable_foreign     NONE
}
----
====
. Redémarrez le `multipathd` démon en exécutant un `systemctl restart multipathd` pour que le nouveau réglage prenne effet.




=== Configuration de l'adaptateur FC Broadcom pour NVMe/FC

.Étapes
. Vérifiez que vous utilisez la carte prise en charge. Pour consulter la liste la plus récente des cartes prises en charge, reportez-vous à la section link:https://mysupport.netapp.com/matrix/["Matrice d'interopérabilité NetApp"^].
+
[listing]
----
# cat /sys/class/scsi_host/host*/modelname
LPe32002-M2
LPe32002-M2
# cat /sys/class/scsi_host/host*/modeldesc
Emulex LightPulse LPe32002-M2 2-Port 32Gb Fibre Channel Adapter
Emulex LightPulse LPe32002-M2 2-Port 32Gb Fibre Channel Adapter
----
. Vérifiez que vous utilisez le micrologiciel et le pilote recommandés pour la boîte de réception Broadcom lpfc. Pour obtenir la liste la plus récente des versions de pilote de carte et de micrologiciel prises en charge, reportez-vous à la section link:https://mysupport.netapp.com/matrix/["Matrice d'interopérabilité NetApp"^].
+
[listing]
----
# cat /sys/class/scsi_host/host*/fwrev
14.0.505.11, sli-4:2:c
14.0.505.11, sli-4:2:c

# cat /sys/module/lpfc/version
0:12.8.0.11
----
. Vérifiez-le `lpfc_enable_fc4_type` est défini sur 3
+
[listing]
----
# cat /sys/module/lpfc/parameters/lpfc_enable_fc4_type
3
----
. Vérifiez que les ports initiateurs sont en fonctionnement et que les LIF cibles sont bien voir.
+
[listing]
----
# cat /sys/class/fc_host/host*/port_name
0x100000109b1c1204
0x100000109b1c1205
# cat /sys/class/fc_host/host*/port_state
Online
Online
# cat /sys/class/scsi_host/host*/nvme_info
NVME Initiator Enabled
XRI Dist lpfc0 Total 6144 IO 5894 ELS 250
NVME LPORT lpfc0 WWPN x100000109b1c1204 WWNN x200000109b1c1204 DID x011d00 ONLINE
NVME RPORT WWPN x203800a098dfdd91 WWNN x203700a098dfdd91 DID x010c07 TARGET DISCSRVC ONLINE
NVME RPORT WWPN x203900a098dfdd91 WWNN x203700a098dfdd91 DID x011507 TARGET DISCSRVC ONLINE
NVME Statistics
LS: Xmt 0000000f78 Cmpl 0000000f78 Abort 00000000
LS XMIT: Err 00000000 CMPL: xb 00000000 Err 00000000
Total FCP Cmpl 000000002fe29bba Issue 000000002fe29bc4 OutIO 000000000000000a
abort 00001bc7 noxri 00000000 nondlp 00000000 qdepth 00000000 wqerr 00000000 err 00000000
FCP CMPL: xb 00001e15 Err 0000d906
NVME Initiator Enabled
XRI Dist lpfc1 Total 6144 IO 5894 ELS 250
NVME LPORT lpfc1 WWPN x100000109b1c1205 WWNN x200000109b1c1205 DID x011900 ONLINE
NVME RPORT WWPN x203d00a098dfdd91 WWNN x203700a098dfdd91 DID x010007 TARGET DISCSRVC ONLINE
NVME RPORT WWPN x203a00a098dfdd91 WWNN x203700a098dfdd91 DID x012a07 TARGET DISCSRVC ONLINE
NVME Statistics
LS: Xmt 0000000fa8 Cmpl 0000000fa8 Abort 00000000
LS XMIT: Err 00000000 CMPL: xb 00000000 Err 00000000
Total FCP Cmpl 000000002e14f170 Issue 000000002e14f17a OutIO 000000000000000a
abort 000016bb noxri 00000000 nondlp 00000000 qdepth 00000000 wqerr 00000000 err 00000000
FCP CMPL: xb 00001f50 Err 0000d9f8
----




=== Activation de la taille d'E/S 1 Mo

.Étapes
. Le `lpfc_sg_seg_cnt` Le paramètre doit être défini sur 256 pour que le pilote lpfc puisse émettre des demandes d'E/S d'une taille maximale de 1 Mo.
+
[listing]
----
# cat /etc/modprobe.d/lpfc.conf
options lpfc lpfc_sg_seg_cnt=256
----
. Exécutez un `dracut -f` puis redémarrez l'hôte. Une fois l'hôte démarré, vérifiez que `lpfc_sg_seg_cnt` est défini sur 256.
+
[listing]
----
# cat /sys/module/lpfc/parameters/lpfc_sg_seg_cnt
256
----




== Configuration de l'adaptateur FC Marvell/QLogic pour NVMe/FC

.Étapes
. Le pilote natif qla2xxx inclus dans le noyau OL 8.5 GA est doté des derniers correctifs en amont, essentiels pour la prise en charge de ONTAP. Vérifiez que vous exécutez les versions du pilote de carte et du micrologiciel prises en charge :
+
[listing]
----
# cat /sys/class/fc_host/host*/symbolic_name
QLE2742 FW:v9.06.02 DVR:v10.02.00.106-k
QLE2742 FW:v9.06.02 DVR:v10.02.00.106-k
----
. La vérification `ql2xnvmeenable` Est défini pour que l'adaptateur Marvell puisse fonctionner comme un initiateur NVMe/FC :
+
[listing]
----
# cat /sys/module/qla2xxx/parameters/ql2xnvmeenable
1
----




== Configuration du protocole NVMe/TCP

Contrairement à NVMe/FC, NVMe/TCP ne propose pas de fonctionnalité de connexion automatique. Cette situation s'explique par deux principales limitations de l'hôte Linux NVMe/TCP :

* Pas de reconnexion automatique après rétablissement des chemins - NVMe/TCP ne peut pas se reconnecter automatiquement à un chemin qui est rétabli au-delà de la valeur par défaut `ctrl-loss-tmo` de 10 minutes après un chemin descendant.
* Pas de connexion automatique pendant le démarrage de l'hôte - NVMe/TCP ne peut pas se connecter automatiquement lors du démarrage de l'hôte.


Pour passer confortablement aux événements de basculement ONTAP tels que les SFO, il est conseillé de définir une période de tentatives plus longue, par exemple 30 minutes, en ajustant le `ctrl_loss_tmo timer`. Voici les instructions :

.Étapes
. Vérifiez si le port initiateur est en mesure de récupérer les données de la page du journal de découverte sur les LIF NVMe/TCP prises en charge :
+
[listing]
----
# nvme discover -t tcp -w 192.168.1.8 -a 192.168.1.51
Discovery Log Number of Records 10, Generation counter 119
=====Discovery Log Entry 0======
trtype: tcp
adrfam: ipv4
subtype: nvme subsystem
treq: not specified
portid: 0
trsvcid: 4420
subnqn: nqn.1992-08.com.netapp:sn.56e362e9bb4f11ebbaded039ea165abc:subsystem.nvme_118_tcp_1
traddr: 192.168.2.56
sectype: none
=====Discovery Log Entry 1======
trtype: tcp
adrfam: ipv4
subtype: nvme subsystem
treq: not specified
portid: 1
trsvcid: 4420
subnqn: nqn.1992-08.com.netapp:sn.56e362e9bb4f11ebbaded039ea165abc:subsystem.nvme_118_tcp_1
traddr: 192.168.1.51
sectype: none
=====Discovery Log Entry 2======
trtype: tcp
adrfam: ipv4
subtype: nvme subsystem
treq: not specified
portid: 0
trsvcid: 4420
subnqn: nqn.1992-08.com.netapp:sn.56e362e9bb4f11ebbaded039ea165abc:subsystem.nvme_118_tcp_2
traddr: 192.168.2.56
sectype: none
...
----
. De la même manière, vérifiez que les autres combos LIF cible-initiateur NVMe/TCP peuvent récupérer intégralement les données de la page du journal de découverte. Par exemple :
+
[listing]
----
# nvme discover -t tcp -w 192.168.1.8 -a 192.168.1.51
# nvme discover -t tcp -w 192.168.1.8 -a 192.168.1.52
# nvme discover -t tcp -w 192.168.2.9 -a 192.168.2.56
# nvme discover -t tcp -w 192.168.2.9 -a 192.168.2.57
----
. Maintenant, exécutez un `nvme connect-all` Sur l'ensemble des LIF cible initiateur NVMe/TCP prises en charge sur l'ensemble des nœuds. Assurez-vous de passer plus longtemps `ctrl_loss_tmo` période (par exemple, 30 minutes, qui peuvent être réglées par `-l 1800`) pendant le `connect-all` de sorte qu'il réessaie pendant une période plus longue en cas de perte de chemin. Par exemple :
+
[listing]
----
# nvme connect-all -t tcp -w 192.168.1.8 -a 192.168.1.51 -l 1800
# nvme connect-all -t tcp -w 192.168.1.8 -a 192.168.1.52 -l 1800
# nvme connect-all -t tcp -w 192.168.2.9 -a 192.168.2.56 -l 1800
# nvme connect-all -t tcp -w 192.168.2.9 -a 192.168.2.57 -l 1800
----




== Validation des protocoles NVMe/FC

.Étapes
. Vérifiez les paramètres NVMe/FC suivants sur l'hôte Oracle Linux 8.5 :
+
[listing]
----
# cat /sys/module/nvme_core/parameters/multipath
Y
# cat /sys/class/nvme-subsystem/nvme-subsys*/model
NetApp ONTAP Controller
NetApp ONTAP Controller
# cat /sys/class/nvme-subsystem/nvme-subsys*/iopolicy
round-robin
round-robin
----
. Vérifier que les espaces de noms sont créés et correctement découverts sur l'hôte :
+
[listing]
----
# nvme list
Node         SN                    Model
---------------------------------------------------------------
/dev/nvme0n1 814vWBNRwf9HAAAAAAAB  NetApp ONTAP Controller
/dev/nvme0n2 814vWBNRwf9HAAAAAAAB  NetApp ONTAP Controller
/dev/nvme0n3 814vWBNRwf9HAAAAAAAB  NetApp ONTAP Controller

Namespace Usage  Format                  FW            Rev
--------------------------------------------------------------
1                85.90 GB / 85.90 GB     4 KiB + 0 B   FFFFFFFF
2                85.90 GB / 85.90 GB     4 KiB + 0 B   FFFFFFFF
3                85.90 GB / 85.90 GB     4 KiB + 0 B   FFFFFFFF
----
. Vérifiez que l'état du contrôleur de chaque chemin est actif et que le statut ANA est correct
+
[listing]
----
# nvme list-subsys /dev/nvme0n1
nvme-subsys0 - NQN=nqn.1992-08.com.netapp:sn.5f5f2c4aa73b11e9967e00a098df41bd:subsystem.nvme_ss_ol_1
\
+- nvme0 fc traddr=nn-0x203700a098dfdd91:pn-0x203800a098dfdd91 host_traddr=nn-0x200000109b1c1204:pn-0x100000109b1c1204 live inaccessible
+- nvme1 fc traddr=nn-0x203700a098dfdd91:pn-0x203900a098dfdd91 host_traddr=nn-0x200000109b1c1204:pn-0x100000109b1c1204 live inaccessible
+- nvme2 fc traddr=nn-0x203700a098dfdd91:pn-0x203a00a098dfdd91 host_traddr=nn-0x200000109b1c1205:pn-0x100000109b1c1205 live optimized
+- nvme3 fc traddr=nn-0x203700a098dfdd91:pn-0x203d00a098dfdd91 host_traddr=nn-0x200000109b1c1205:pn-0x100000109b1c1205 live optimized
----
. Vérifiez que le plug-in NetApp affiche les valeurs appropriées pour chaque système d'espace de noms ONTAP
+
[listing]
----
# nvme netapp ontapdevices -o column
Device       Vserver  Namespace Path
-----------------------------------
/dev/nvme0n1  vs_ol_nvme  /vol/ol_nvme_vol_1_1_0/ol_nvme_ns
/dev/nvme0n2  vs_ol_nvme  /vol/ol_nvme_vol_1_0_0/ol_nvme_ns
/dev/nvme0n3  vs_ol_nvme  /vol/ol_nvme_vol_1_1_1/ol_nvme_ns

NSID    UUID                                   Size
-----------------------------------------------------
1       72b887b1-5fb6-47b8-be0b-33326e2542e2   85.90GB
2       04bf9f6e-9031-40ea-99c7-a1a61b2d7d08   85.90GB
3       264823b1-8e03-4155-80dd-e904237014a4   85.90GB

# nvme netapp ontapdevices -o json
{
"ONTAPdevices" : [
    {
        "Device" : "/dev/nvme0n1",
        "Vserver" : "vs_ol_nvme",
        "Namespace_Path" : "/vol/ol_nvme_vol_1_1_0/ol_nvme_ns",
        "NSID" : 1,
        "UUID" : "72b887b1-5fb6-47b8-be0b-33326e2542e2",
        "Size" : "85.90GB",
        "LBA_Data_Size" : 4096,
        "Namespace_Size" : 20971520
    },
    {
        "Device" : "/dev/nvme0n2",
        "Vserver" : "vs_ol_nvme",
        "Namespace_Path" : "/vol/ol_nvme_vol_1_0_0/ol_nvme_ns",
        "NSID" : 2,
        "UUID" : "04bf9f6e-9031-40ea-99c7-a1a61b2d7d08",
        "Size" : "85.90GB",
        "LBA_Data_Size" : 4096,
        "Namespace_Size" : 20971520
      },
      {
         "Device" : "/dev/nvme0n3",
         "Vserver" : "vs_ol_nvme",
         "Namespace_Path" : "/vol/ol_nvme_vol_1_1_1/ol_nvme_ns",
         "NSID" : 3,
         "UUID" : "264823b1-8e03-4155-80dd-e904237014a4",
         "Size" : "85.90GB",
         "LBA_Data_Size" : 4096,
         "Namespace_Size" : 20971520
       },
  ]
}
----




== Dépannage

Avant de commencer tout dépannage concernant une défaillance de NVMe/FC, veillez à toujours exécuter une configuration conforme aux spécifications de IMT. Puis passez aux étapes suivantes ci-dessous pour déboguer tout problème côté hôte ici.



=== consignation détaillée lpfc

Voici la liste des masques binaires de journalisation pour le pilote lpfc disponibles pour NVMe/FC, comme illustré à la `drivers/scsi/lpfc/lpfc_logmsg.h`:

[listing]
----
#define LOG_NVME 0x00100000 /* NVME general events. */
#define LOG_NVME_DISC 0x00200000 /* NVME Discovery/Connect events. */
#define LOG_NVME_ABTS 0x00400000 /* NVME ABTS events. */
#define LOG_NVME_IOERR 0x00800000 /* NVME IO Error events. */
----
Vous pouvez donc régler le `lpfc_log_verbose` réglage du conducteur (ajouté à la ligne lpfc à `/etc/modprobe.d/lpfc.conf`) À n'importe laquelle des valeurs ci-dessus pour enregistrer des événements NVMe/FC du point de vue du pilote lpfc. Puis recréez les initiramfs en cours d'exécution `dracut -f` puis redémarrez l'hôte. Après le redémarrage, vérifiez que la consignation détaillée s'applique en vérifiant les éléments suivants à l'aide des éléments ci-dessus `LOG_NVME_DISC bitmask` par exemple :

[listing]
----
# cat /etc/modprobe.d/lpfc.conf
lpfc_enable_fc4_type=3 lpfc_log_verbose=0xf00083
# cat /sys/module/lpfc/parameters/lpfc_log_verbose
15728771
----


=== consignation détaillée qla2xxx

Il n'y a pas de journalisation qla2xxx spécifique similaire pour NVMe/FC que pour le pilote lpfc. Par conséquent, vous pouvez définir le niveau de consignation général qla2xxx en procédant comme suit :

.Étapes
. Ajoutez le `ql2xextended_error_logging=0x1e400000` valeur du fichier de confirmation modprobe qla2xxx correspondant.
. Recréez le `initramfs` en cours d'exécution `dracut -f` puis redémarrez l'hôte.
. Après le redémarrage, vérifiez que la journalisation détaillée est appliquée comme suit :
+
[listing]
----
# cat /etc/modprobe.d/qla2xxx.conf
options qla2xxx ql2xnvmeenable=1 ql2xextended_error_logging=0x1e400000
# cat /sys/module/qla2xxx/parameters/ql2xextended_error_logging
507510784
----




=== Erreurs et solutions nvme-cli courantes

Les erreurs affichées par le `nvme-cli` commande au cours de la découverte nvme, les opérations nvme connect ou nvme connect-toutes les opérations et les solutions de contournement sont affichées dans le tableau suivant :

[cols="20, 20, 50"]
|===
| Erreurs affichées par l'interface de ligne de commandes nvme | Cause probable | Solution de contournement 


| `Failed to write to /dev/nvme-fabrics: Invalid argument` erreur affichée lors de la découverte nvme, de la connexion nvme ou de la connexion nvme | Syntaxe incorrecte | Assurez-vous d'utiliser la syntaxe correcte pour les commandes nvme données. 


| `Failed to write to /dev/nvme-fabrics: No such file or directory` erreur affichée lors de la découverte nvme, de la connexion nvme ou de la connexion nvme | Plusieurs problèmes peuvent déclencher cette action. Certains des cas les plus courants sont : vous avez transmis des arguments incorrects aux commandes nvme données.  a| 
* Veillez à avoir transmis les arguments appropriés (par exemple, la chaîne WWNN, la chaîne WWPN, etc.) pour les commandes indiquées.
* Si les arguments sont corrects, mais que vous voyez toujours cette erreur, vérifiez si `/sys/class/scsi_host/host*/nvme_info` Le résultat de cette commande est correct avec l'intiator NVMe affiché comme `Enabled` Les LIF cible NVMe/FC s'affichent correctement ici sous les sections des ports distants. Par exemple :
+
[listing]
----
# cat /sys/class/scsi_host/host*/nvme_info
NVME Initiator Enabled
NVME LPORT lpfc0 WWPN x10000090fae0ec9d WWNN x20000090fae0ec9d DID x012000 ONLINE
NVME RPORT WWPN x200b00a098c80f09 WWNN x200a00a098c80f09 DID x010601 TARGET DISCSRVC ONLINE
NVME Statistics
LS: Xmt 0000000000000006 Cmpl 0000000000000006
FCP: Rd 0000000000000071 Wr 0000000000000005 IO 0000000000000031
Cmpl 00000000000000a6 Outstanding 0000000000000001
NVME Initiator Enabled
NVME LPORT lpfc1 WWPN x10000090fae0ec9e WWNN x20000090fae0ec9e DID x012400 ONLINE
NVME RPORT WWPN x200900a098c80f09 WWNN x200800a098c80f09 DID x010301 TARGET DISCSRVC ONLINE
NVME Statistics
LS: Xmt 0000000000000006 Cmpl 0000000000000006
FCP: Rd 0000000000000073 Wr 0000000000000005 IO 0000000000000031
Cmpl 00000000000000a8 Outstanding 0000000000000001
----
* Si les LIF cibles n'apparaissent pas comme ci-dessus dans la `nvme_info` sortie, vérifier le `/var/log/` messages et `dmesg` Sortie pour toute défaillance NVMe/FC suspecte, puis rapport ou correction en conséquence.




| `No discovery log entries to fetch` erreur affichée lors de la découverte nvme, de la connexion nvme ou de la connexion nvme | Ce message d'erreur s'affiche généralement si le `/etc/nvme/hostnqn` La chaîne n'a pas été ajoutée au sous-système correspondant de la baie NetApp. Ou une mauvaise réponse `hostnqn` la chaîne a été ajoutée au sous-système respectif. | Assurez-vous que le système est exact `/etc/nvme/hostnqn` La chaîne est ajoutée au sous-système correspondant de la baie NetApp (vérifiez via le `vserver nvme subsystem host show` commande). 


| `Failed to write to /dev/nvme-fabrics: Operation already in progress` affiché lors de la découverte nvme, nvme connect ou nvme connect-all | Ce message d'erreur s'affiche si les associations de contrôleur ou l'opération spécifiée sont déjà créées ou en cours de création. Cela peut se produire dans le cadre des scripts de connexion automatique installés ci-dessus. | Aucune. Pour découvrir nvme, essayez d'exécuter cette commande après un certain temps. Peut-être pour nvme connecter et se connecter-tout, exécutez un `nvme list` pour vérifier que les périphériques d'espace de noms sont déjà créés et affichés sur l'hôte. 
|===


=== Quand contacter le support technique

Si vous rencontrez toujours des problèmes, veuillez rassembler les fichiers et les sorties de commande suivants et contacter le support technique pour plus de triage :

[listing]
----
cat /sys/class/scsi_host/host*/nvme_info
/var/log/messages
dmesg
nvme discover output as in:
nvme discover --transport=fc --traddr=nn-0x200a00a098c80f09:pn-0x200b00a098c80f09 --host-traddr=nn-0x20000090fae0ec9d:pn-0x10000090fae0ec9d
nvme list
nvme list-subsys /dev/nvmeXnY
----


== Problèmes connus et solutions

Aucune.
